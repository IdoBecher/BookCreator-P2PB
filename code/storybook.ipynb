{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ⬜ Cell 0 - Disk Clearing (run if we get warning: \"Disk is almost full\")\n",
        "!rm -rf ~/.cache/huggingface/hub\n",
        "!pip cache purge -q\n",
        "!find . -name \"tmp_scene_*\" -delete\n",
        "!find . -name \"scene_variants\" -type d -exec rm -rf {} +\n",
        "!df -h /"
      ],
      "metadata": {
        "id": "yOqjr4s8-k8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⬜ Cell 1 - install base libraries\n",
        "!pip -q install --upgrade transformers accelerate bitsandbytes diffusers"
      ],
      "metadata": {
        "id": "A5iV__4U-mZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# 🔑 put in Hugging Face token\n",
        "login(\"put in Hugging Face token\")"
      ],
      "metadata": {
        "id": "JP5QSqnb5uXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⬜ Cell 2 - loading Mistral-7B-Instruct-v0.2 ב-4bit\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"auto\",          # Accelerate - deals with the mapping\n",
        "    torch_dtype=\"auto\",\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "\n",
        "text_gen = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"✅ pipeline ready. device map:\", getattr(model, \"hf_device_map\", \"n/a\"))"
      ],
      "metadata": {
        "id": "Btx6cQnk-sQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Minimal helpers for the \"last Cell 3\" =====\n",
        "# Uses globals from Cell 2: text_gen OR (model, tokenizer)\n",
        "\n",
        "import json, textwrap, time, re\n",
        "from typing import List\n",
        "\n",
        "def llm(user_prompt: str, max_new_tokens=150, temperature=1.2, top_p=0.95, rep=1.12) -> str:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You write gentle, visual picture-book text for children. Avoid violence or scary content.\"},\n",
        "        {\"role\": \"user\",   \"content\": user_prompt}\n",
        "    ]\n",
        "    chat_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    out = text_gen(\n",
        "        chat_text,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=rep,\n",
        "        return_full_text=False,\n",
        "    )[0][\"generated_text\"]\n",
        "    return out.strip()\n",
        "\n",
        "def first_nonempty_line(s: str) -> str:\n",
        "    for ln in s.splitlines():\n",
        "        ln = ln.strip().lstrip(\"-*•>#\").strip()\n",
        "        low = ln.lower()\n",
        "        if ln and not low.startswith((\"assistant:\", \"user:\", \"system:\", \"scene \", \"scene:\", \"1.\", \"2.\", \"3.\", \"4.\", \"5.\", \"###\")):\n",
        "            return ln\n",
        "    return s.strip()\n",
        "\n",
        "def _call_llm(prompt: str,\n",
        "              max_new_tokens: int = 180,\n",
        "              temperature: float = 0.9,\n",
        "              top_p: float = 0.95,\n",
        "              rep: float = 1.05) -> str:\n",
        "    \"\"\"\n",
        "    Calls the LLM using objects created in תא 2.\n",
        "    Prefers `text_gen` pipeline; falls back to (model, tokenizer).\n",
        "    \"\"\"\n",
        "    if \"text_gen\" in globals() and text_gen is not None:\n",
        "        out = text_gen(\n",
        "            prompt,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p\n",
        "        )\n",
        "        return out[0][\"generated_text\"]\n",
        "\n",
        "    if \"model\" in globals() and \"tokenizer\" in globals() and model is not None and tokenizer is not None:\n",
        "        from transformers import TextStreamer\n",
        "        import torch\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        if hasattr(model, \"device\"):\n",
        "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "        streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "        gen = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            streamer=streamer\n",
        "        )\n",
        "        return tokenizer.decode(gen[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "    raise RuntimeError(\"❌ Missing backend. Run תא 2 so text_gen/model/tokenizer exist.\")\n",
        "\n",
        "def unglue_common_articles(s: str) -> str:\n",
        "    \"\"\"Fixes glued words like 'meadowthe' -> 'meadow the' (keeps exceptions).\"\"\"\n",
        "    exceptions = {\"lathe\",\"scythe\",\"breathe\",\"writhe\",\"loathe\",\"sheathe\",\"wreathe\",\"clothe\"}\n",
        "    def repl(m):\n",
        "        left, art = m.group(1), m.group(2)\n",
        "        token = (left + art).lower()\n",
        "        return left + art if token in exceptions else left + \" \" + art\n",
        "    return re.sub(r\"([A-Za-z]{3,})(?:(the|and|in|on|at)\\b)\", repl, s)\n",
        "\n",
        "# Gentle cleanups the last Cell 3 uses\n",
        "REPLACEMENTS = [\n",
        "    (r\"\\bstriped bananas?\\b\", \"ripe bananas\"),\n",
        "    (r\"\\bzesty lemon\\b\", \"bright yellow\"),\n",
        "    (r\"\\brainbow fairies\\b\", \"butterflies\"),\n",
        "    (r\"\\bglowing fairy dust\\b\", \"soft sparkling dust\"),\n",
        "    (r\"\\bmeadowthe\\b\", \"meadow the\"),\n",
        "    (r\"\\bpurrs quietly quietly\\b\", \"purrs quietly\"),\n",
        "]\n",
        "\n",
        "def apply_replacements(s: str) -> str:\n",
        "    for pat, rep in REPLACEMENTS:\n",
        "        s = re.sub(pat, rep, s, flags=re.IGNORECASE)\n",
        "    return s\n",
        "\n",
        "def fantasy_ok(title: str) -> bool:\n",
        "    \"\"\"Used by Cell 3 to allow a bit of 'magical' flavor in titles.\"\"\"\n",
        "    return re.search(r\"\\b(magic|magical|enchanted|spell|wizard|dragon|fairy|fairies)\\b\", title.lower()) is not None\n"
      ],
      "metadata": {
        "id": "EMisubH_XNxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== Cell 3 - input, improvement, and general conistancy/solo ======================\n",
        "import re, json, textwrap\n",
        "from typing import List\n",
        "\n",
        "\n",
        "SUBJECT_LOCK       = True    # keep the same character between scenes\n",
        "SOLO_SUBJECT       = True    # excludes additional characters\n",
        "START_WITH_SUBJECT = False   # if true, the sentence will start with the character\n",
        "\n",
        "_HEAD_ALIASES = {\n",
        "    \"dog\":   [\"puppy\", \"pup\"],  \"puppy\": [\"dog\", \"pup\"],\n",
        "    \"cat\":   [\"kitten\", \"kitty\"], \"kitten\": [\"cat\", \"kitty\"],\n",
        "    \"horse\": [\"foal\", \"colt\", \"filly\", \"pony\"], \"foal\": [\"horse\", \"colt\", \"filly\"],\n",
        "    \"bird\":  [\"chick\"],  \"child\": [\"boy\", \"girl\", \"kid\"], \"robot\": []\n",
        "}\n",
        "\n",
        "def _pluralize(w: str) -> str:\n",
        "    w = w.lower()\n",
        "    if w.endswith(\"y\") and len(w) > 1 and w[-2] not in \"aeiou\":\n",
        "        return w[:-1] + \"ies\"\n",
        "    if w.endswith((\"s\",\"x\",\"z\",\"ch\",\"sh\")):\n",
        "        return w + \"es\"\n",
        "    return w + \"s\"\n",
        "\n",
        "def build_aliases(head: str):\n",
        "    #builds a set of basic aliases from the head_noun\n",
        "    h = (head or \"character\").lower().strip()\n",
        "    alts = {h, _pluralize(h)}\n",
        "    for base, vs in _HEAD_ALIASES.items():\n",
        "        fam = {base, _pluralize(base), *vs, *(_pluralize(v) for v in vs)}\n",
        "        if h in fam:\n",
        "            alts |= fam\n",
        "            break\n",
        "    return alts\n",
        "\n",
        "def normalize_subject_mentions_generic(text: str, head: str) -> str:\n",
        "    \"\"\"\n",
        "    כל NP כמו 'a/an/the ... <alias>' (לא 'the same') → 'the same <head>'.\n",
        "    גם 'another/a different <alias>' → 'the same <head>'.\n",
        "    \"\"\"\n",
        "    aliases = \"|\".join(sorted(map(re.escape, build_aliases(head))))\n",
        "    out = re.sub(rf\"\\b(another|a\\s*different)\\s+({aliases})\\b\",\n",
        "                 f\"the same {head}\", text, flags=re.I)\n",
        "    out = re.sub(rf\"\\b(a|an|the)\\s+(?:(?!same)\\w+\\s+){{0,5}}({aliases})\\b\",\n",
        "                 f\"the same {head}\", out, flags=re.I)\n",
        "    out = re.sub(r\"\\s{2,}\", \" \", out)\n",
        "    out = re.sub(r\"\\s+([,.!?;:])\", r\"\\1\", out)\n",
        "    return out.strip()\n",
        "\n",
        "def violates_subject_lock_generic(text: str, head: str) -> bool:\n",
        "    \"\"\"דוחה אם עדיין נראה שמוצגות דמויות/מופעים שונים של אותו סוג.\"\"\"\n",
        "    aliases = \"|\".join(sorted(map(re.escape, build_aliases(head))))\n",
        "    if re.search(rf\"\\b(another|a\\s*different)\\s+({aliases})\\b\", text, flags=re.I):\n",
        "        return True\n",
        "    # כמה מופעי NP אחרי הנירמול? מתירים עד שניים: פתיחה + 'the same ...'\n",
        "    np_hits = re.findall(rf\"\\b(a|an|the)\\s+(?:same\\s+)?({aliases})\\b\", text, flags=re.I)\n",
        "    return len(np_hits) > 2\n",
        "\n",
        "# ---------- Solo: animative word bank + check ----------\n",
        "ANIMATE_WORDS = {\n",
        "    # humans\n",
        "    \"man\",\"woman\",\"boy\",\"girl\",\"child\",\"kid\",\"person\",\"people\",\"crowd\",\"vendor\",\"seller\",\"keeper\",\n",
        "    \"ranger\",\"stranger\",\"friend\",\"friends\",\"family\",\"mother\",\"father\",\"grandma\",\"grandpa\",\"teacher\",\n",
        "    # common animals\n",
        "    \"dog\",\"dogs\",\"puppy\",\"puppies\",\"cat\",\"cats\",\"kitten\",\"kittens\",\"bird\",\"birds\",\"horse\",\"horses\",\n",
        "    \"foal\",\"foals\",\"cow\",\"cows\",\"sheep\",\"goat\",\"goats\",\"rabbit\",\"rabbits\",\"bunny\",\"bunnies\",\"mouse\",\"mice\",\n",
        "    \"bear\",\"bears\",\"fox\",\"foxes\",\"deer\",\"duck\",\"ducks\",\"swan\",\"swans\",\"fish\",\"fishes\",\"frog\",\"frogs\",\n",
        "    \"butterfly\",\"butterflies\",\"bee\",\"bees\",\"squirrel\",\"squirrels\",\"dragon\",\"dragons\",\"wizard\",\"wizards\"\n",
        "}\n",
        "_DOG_BREEDS = r\"(golden\\s+retriever|labrador(?:\\s+retriever)?|german\\s+shepherd|pomeranian|poodle|beagle|bulldog|husky|malamute|akita|boxer|dachshund|doberman|greyhound|whippet|great\\s+dane|chihuahua|pug|corgi|shiba|samoyed|pointer|spaniel|terrier|rottweiler|mastiff|bern(?:e|ese)\\s+mountain|australian\\s+shepherd|border\\s+collie|bichon|shih\\s*tzu|maltese|newfoundland|saint\\s+bernard|weimaraner|vizsla|sheltie)\"\n",
        "BREED_RE = re.compile(rf\"\\b{_DOG_BREEDS}\\b\", re.I)\n",
        "\n",
        "def violates_solo_subject(text: str, head: str) -> bool:\n",
        "    \"\"\"\n",
        "    true if there is variation for the head_noun in the text.\n",
        "    \"\"\"\n",
        "    low = text.lower()\n",
        "\n",
        "    if head.lower() in {\"dog\",\"puppy\",\"pup\"} and BREED_RE.search(low):\n",
        "        return True\n",
        "\n",
        "    aliases = {a.lower() for a in build_aliases(head)}\n",
        "    tokens = re.findall(r\"[a-z][-\\w']*\", low)\n",
        "    for t in tokens:\n",
        "        if t in ANIMATE_WORDS and t not in aliases and t != \"it\":\n",
        "            return True\n",
        "\n",
        "    anim_union = \"|\".join(sorted(ANIMATE_WORDS))\n",
        "    if re.search(rf\"\\b(with|and|alongside|near|beside)\\s+(a|an|the)\\s+(?:\\w+\\s+){{0,3}}({anim_union})\\b\", low):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# ---------- General text helpers ----------\n",
        "COLOR_WORDS = {\n",
        "    \"red\",\"crimson\",\"scarlet\",\"pink\",\"rose\",\"orange\",\"amber\",\"gold\",\"golden\",\"yellow\",\"saffron\",\n",
        "    \"green\",\"emerald\",\"jade\",\"blue\",\"azure\",\"cerulean\",\"navy\",\"indigo\",\"purple\",\"violet\",\"lilac\",\n",
        "    \"white\",\"black\",\"brown\",\"silver\",\"grey\",\"gray\",\"rainbow\"\n",
        "}\n",
        "\n",
        "def squeeze_enums(text: str, max_items=2) -> str:\n",
        "    def _shrink(chunk: str) -> str:\n",
        "        items = [w.strip() for w in chunk.split(\",\")]\n",
        "        items = [i for i in items if i][:max_items]\n",
        "        return \", \".join(items)\n",
        "    return re.sub(r\"(?:\\s*,\\s*[^,]{2,}){3,}\", lambda m: _shrink(m.group(0)), text)\n",
        "\n",
        "def limit_colors(text: str, max_colors=2) -> str:\n",
        "    tokens, seen, out = text.split(), 0, []\n",
        "    for t in tokens:\n",
        "        key = t.lower().strip(\",.;:!?\")\n",
        "        if key in COLOR_WORDS:\n",
        "            if seen >= max_colors:\n",
        "                continue\n",
        "            seen += 1\n",
        "        out.append(t)\n",
        "    return \" \".join(out)\n",
        "\n",
        "def first_nonempty_line(s: str) -> str:\n",
        "    for line in s.splitlines():\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            return line\n",
        "    return s.strip()\n",
        "\n",
        "def unglue_common_articles(s: str) -> str:\n",
        "    exceptions = {\"lathe\",\"scythe\",\"breathe\",\"writhe\",\"loathe\",\"sheathe\",\"wreathe\",\"clothe\"}\n",
        "    def repl(m):\n",
        "        left, art = m.group(1), m.group(2)\n",
        "        return left + art if (left+art).lower() in exceptions else left + \" \" + art\n",
        "    return re.sub(r\"([A-Za-z]{3,})(?:(the|and|in|on|at)\\b)\", repl, s)\n",
        "\n",
        "def compact_scene(line: str) -> str:\n",
        "    # take the first sentence only\n",
        "    first = re.split(r'(?<=[.!?])\\s+', line.strip())[0].strip()\n",
        "    line = re.sub(r\"\\s+\", \" \", first)\n",
        "    line = squeeze_enums(line, max_items=2)\n",
        "    line = limit_colors(line, max_colors=2)\n",
        "    line = re.sub(r\"\\s*,\\s*(?=[.!?]|$)\", \"\", line)\n",
        "    words = line.split()\n",
        "    if len(words) > 28:\n",
        "        line = \" \".join(words[:28]).rstrip(\",\") + \".\"\n",
        "    if not re.search(r'[.!?]$', line):\n",
        "        line += \".\"\n",
        "    return line\n",
        "\n",
        "# Generalized wording fix\n",
        "REPLACEMENTS = [\n",
        "    (r\"\\bstriped bananas?\\b\", \"ripe bananas\"),\n",
        "    (r\"\\bzesty lemon\\b\", \"bright yellow\"),\n",
        "    (r\"\\bglowing fairy dust\\b\", \"soft sparkling dust\"),\n",
        "    (r\"\\bmeadowthe\\b\", \"meadow the\"),\n",
        "    (r\"\\bpurrs quietly quietly\\b\", \"purrs quietly\"),\n",
        "]\n",
        "def deweird(s: str) -> str:\n",
        "    out = s\n",
        "    for pat, rep in REPLACEMENTS:\n",
        "        out = re.sub(pat, rep, out, flags=re.IGNORECASE)\n",
        "    out = re.sub(r\"\\bpaw(ing)? at (a )?(crimson|scarlet|red|orange|yellow|green|blue|purple)\\b\",\n",
        "                 r\"paw\\1 at a \\3 leaf\", out, flags=re.IGNORECASE)\n",
        "    return out\n",
        "\n",
        "EAT_FIXES = [\n",
        "    (r\"\\b(nibbl\\w*|munch\\w*|snack\\w*) on (water lily pads|lily pads|petals|flowers|blossoms|leaves)\\b\",\n",
        "     r\"sniffs the \\2\"),\n",
        "    (r\"\\b(nibbl\\w*|munch\\w*|snack\\w*) on (clover blossoms|raspberries?)\\b\",\n",
        "     r\"sniffs the \\2\"),\n",
        "]\n",
        "def deweird_eating(s: str) -> str:\n",
        "    out = s\n",
        "    for pat, rep in EAT_FIXES:\n",
        "        out = re.sub(pat, rep, out, flags=re.IGNORECASE)\n",
        "    return out\n",
        "\n",
        "def is_fragment_end(s: str) -> bool:\n",
        "    bad_ends = {\"the\",\"a\",\"an\",\"and\",\"or\",\"of\",\"to\",\"in\",\"on\",\"at\",\"with\",\"from\",\"by\",\"into\",\"over\",\"under\",\"near\",\"while\"}\n",
        "    last = re.sub(r\"[^A-Za-z]+$\", \"\", s.strip().lower().split()[-1])\n",
        "    return last in bad_ends\n",
        "\n",
        "def fix_trailing_fragment(s: str) -> str:\n",
        "    s = re.sub(r\"\\.{3,}$\", \".\", s)\n",
        "    s = re.sub(r\"\\s*\\.\\.+\\s*$\", \".\", s)\n",
        "    if is_fragment_end(s):\n",
        "        cut = re.split(r'(?<=[.!?])\\s+', s.strip())\n",
        "        if cut and re.search(r'[.!?]$', cut[0]): s = cut[0]\n",
        "        else:\n",
        "            words = s.strip().split()\n",
        "            while words and is_fragment_end(\" \".join(words)):\n",
        "                words = words[:-1]\n",
        "            s = \" \".join(words)\n",
        "    if not re.search(r'[.!?]$', s.strip()):\n",
        "        s = s.rstrip(\",;:\") + \".\"\n",
        "    return s\n",
        "\n",
        "def fantasy_ok(title: str) -> bool:\n",
        "    return re.search(r\"\\b(magic|magical|enchanted|spell|wizard|dragon|fairy|fairies)\\b\", title.lower()) is not None\n",
        "\n",
        "# ---------- Reading the language model from the last cell ----------\n",
        "def _call_llm(prompt: str, max_new_tokens=160, temperature=0.9, top_p=0.9, rep=1.08) -> str:\n",
        "    if \"llm\" in globals() and callable(globals()[\"llm\"]):\n",
        "        return globals()[\"llm\"](prompt, max_new_tokens=max_new_tokens,\n",
        "                                temperature=temperature, top_p=top_p, rep=rep)\n",
        "    if \"text_gen\" in globals():\n",
        "        try:\n",
        "            out = text_gen(prompt, max_new_tokens=max_new_tokens,\n",
        "                           do_sample=True, temperature=temperature, top_p=top_p,\n",
        "                           return_full_text=False)\n",
        "            return out[0][\"generated_text\"]\n",
        "        except TypeError:\n",
        "            out = text_gen(prompt, max_new_tokens=max_new_tokens,\n",
        "                           do_sample=True, temperature=temperature, top_p=top_p)\n",
        "            txt = out[0][\"generated_text\"]\n",
        "            return txt[len(prompt):].lstrip() if txt.startswith(prompt) else txt\n",
        "    if \"model\" in globals() and \"tokenizer\" in globals():\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            [{\"role\":\"user\",\"content\":prompt}],\n",
        "            add_generation_prompt=True, return_tensors=\"pt\"\n",
        "        ).to(model.device)\n",
        "        gen = model.generate(\n",
        "            inputs, max_new_tokens=max_new_tokens,\n",
        "            do_sample=True, temperature=temperature, top_p=top_p,\n",
        "            repetition_penalty=rep, eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        return tokenizer.decode(gen[0, inputs.shape[1]:], skip_special_tokens=True)\n",
        "    raise RuntimeError(\"❌ No language model found. Run Cell 2 before running Cell 3\")\n",
        "\n",
        "# ---------- שיפור כותרת/דמות ----------\n",
        "_JSON_OBJ = re.compile(r\"\\{.*?\\}\", re.S)\n",
        "\n",
        "def improve_seed(topic: str, char_desc: str, tries=3):\n",
        "    for _ in range(tries):\n",
        "        prompt = textwrap.dedent(f\"\"\"\n",
        "            Improve the following children's story seed.\n",
        "            Return ONLY compact JSON: {{\"title\": \"...\", \"character\": \"...\"}}\n",
        "            - Keep it child-friendly.\n",
        "            - Title: 3–7 words, no quotes.\n",
        "            - Character: one concise line (≤ 16 words), present tense.\n",
        "\n",
        "            Topic: \"{topic}\"\n",
        "            Character: \"{char_desc}\"\n",
        "        \"\"\").strip()\n",
        "        raw = _call_llm(prompt, max_new_tokens=120)\n",
        "        m = _JSON_OBJ.search(raw)\n",
        "        if m:\n",
        "            try:\n",
        "                j = json.loads(m.group(0))\n",
        "                title = (j.get(\"title\") or topic).strip().strip('\"')\n",
        "                char  = (j.get(\"character\") or char_desc).strip().strip('\"')\n",
        "                if title and char:\n",
        "                    return title, char\n",
        "            except Exception:\n",
        "                pass\n",
        "    return topic.strip(), char_desc.strip()\n",
        "\n",
        "# ---------- חילוץ ביטוי קנוני + שם עצם ----------\n",
        "def canonicalize_identity(topic: str, char_desc: str):\n",
        "    prompt = textwrap.dedent(f\"\"\"\n",
        "        From the following children's-story seed, extract a short canonical identity phrase for the MAIN CHARACTER.\n",
        "        Return ONLY JSON:\n",
        "        {{\"canon\":\"<2–6 words noun phrase to refer to the character, may include a name>\", \"noun\":\"<head noun>\"}}\n",
        "\n",
        "        Topic: \"{topic}\"\n",
        "        Character: \"{char_desc}\"\n",
        "    \"\"\").strip()\n",
        "    raw = _call_llm(prompt, max_new_tokens=80, temperature=0.2, top_p=0.9)\n",
        "    canon, noun = None, None\n",
        "    m = _JSON_OBJ.search(raw)\n",
        "    if m:\n",
        "        try:\n",
        "            j = json.loads(m.group(0))\n",
        "            canon = j.get(\"canon\")\n",
        "            noun  = j.get(\"noun\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not canon:\n",
        "        words = re.findall(r\"[A-Za-z][-\\w']*\", char_desc)\n",
        "        canon = \" \".join(words[:6]) if words else \"main character\"\n",
        "    if not noun:\n",
        "        m = re.search(r\"\\b(boy|girl|child|kid|kitten|cat|puppy|dog|foal|horse|robot|fox|bear|rabbit|mouse|dragon|wizard|bird)\\b\",\n",
        "                      (char_desc + \" \" + canon).lower())\n",
        "        noun = m.group(1) if m else \"character\"\n",
        "    return canon.strip().strip('\"').replace(\"  \", \" \"), noun.strip()\n",
        "\n",
        "def ensure_canon_once(s: str, canon_phrase: str, *, at_begin: bool = False) -> str:\n",
        "    \"\"\"\n",
        "    makes sure the canonical phrase only appears once\n",
        "    \"\"\"\n",
        "    pat = re.compile(re.escape(canon_phrase), re.I)\n",
        "    if pat.search(s):\n",
        "        s = pat.sub(\"__SUBJ__\", s, count=1)\n",
        "        s = pat.sub(\"\", s)\n",
        "        return s.replace(\"__SUBJ__\", canon_phrase)\n",
        "\n",
        "    if at_begin:\n",
        "        return f\"{canon_phrase} — {s.lstrip()}\"\n",
        "\n",
        "    if re.search(r'[.!?]\\s*$', s):\n",
        "        return re.sub(r'\\s*[.!?]\\s*$', f\" — {canon_phrase}.\", s)\n",
        "    else:\n",
        "        return f\"{s} — {canon_phrase}\"\n",
        "\n",
        "# ---------- יצירת סצנה אחת ----------\n",
        "def ask_scene(title: str, char_desc: str, canon_phrase: str, head_noun: str,\n",
        "              used: List[str], attempt=0):\n",
        "    min_words = 10 if attempt == 0 else 8\n",
        "    forbid = \" | \".join(used) if used else \"none\"\n",
        "    realism = \"\" if fantasy_ok(title) else \"• Keep it realistic; no fantasy or mythical creatures.\\n\"\n",
        "\n",
        "    inclusion = (f'• Include this subject phrase ONCE at the beginning: \"{canon_phrase}\" (do NOT change species/name/number).'\n",
        "                 if START_WITH_SUBJECT else\n",
        "                 f'• Include this subject phrase EXACTLY ONCE (anywhere): \"{canon_phrase}\" (do NOT change species/name/number).')\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Write EXACTLY ONE sentence for a child-friendly picture-book scene for “{title}”.\n",
        "• 18–28 words, present tense, concrete visuals (who/where/action).\n",
        "• Use at most TWO colour words; avoid long comma lists.\n",
        "{realism}{inclusion}\n",
        "• Single subject only (one {head_noun}); no duplicates, no crowds.\n",
        "• No other characters at all — no people and no other animals or {head_noun}s; only scenery and simple objects (ball, leaf, pond, tree, bench, path).\n",
        "• Do NOT start with “Scene”, numbers, or bullets.\n",
        "• Avoid repeating phrases from: {forbid}\n",
        "• End cleanly with a period. Then write <END> on its own line.\n",
        "• Keep the SAME subject across scenes: if you mention it again, write “the same {head_noun}” or use a pronoun — never “another” or “a different” {head_noun}.\n",
        "\n",
        "Main character: {char_desc}\n",
        "Scene:\n",
        "\"\"\".strip()\n",
        "\n",
        "    raw = _call_llm(prompt, max_new_tokens=120, temperature=0.95, top_p=0.92, rep=1.12)\n",
        "    text = raw.split(\"<END>\")[0].strip()\n",
        "    line = first_nonempty_line(text)\n",
        "\n",
        "    # cleaning and polishing\n",
        "    line = compact_scene(line)\n",
        "    line = unglue_common_articles(line)\n",
        "    line = deweird(line)\n",
        "    line = deweird_eating(line)\n",
        "    line = ensure_canon_once(line, canon_phrase, at_begin=START_WITH_SUBJECT)\n",
        "    line = fix_trailing_fragment(line)\n",
        "\n",
        "    # keeping the same subject through scenes\n",
        "    if SUBJECT_LOCK:\n",
        "        line = normalize_subject_mentions_generic(line, head=head_noun)\n",
        "        if violates_subject_lock_generic(line, head=head_noun):\n",
        "            return None\n",
        "\n",
        "    # exclude extra characters\n",
        "    if SOLO_SUBJECT and violates_solo_subject(line, head_noun):\n",
        "        return None\n",
        "\n",
        "    if len(line.split()) >= min_words and all(line.lower() != s.lower() for s in used):\n",
        "        return line\n",
        "    return None\n",
        "\n",
        "def generate_scenes(title: str, char_desc: str, canon_phrase: str, head_noun: str, tries=8):\n",
        "    used, scenes = [], []\n",
        "    for _ in range(5):\n",
        "        line = None\n",
        "        for attempt in range(tries):\n",
        "            line = ask_scene(title, char_desc, canon_phrase, head_noun, used, attempt=attempt)\n",
        "            if line:\n",
        "                used.append(line); scenes.append(line); break\n",
        "        if not line:\n",
        "            raise ValueError(\"The model failed to make 5 proper scenes after a number of tries\")\n",
        "    return scenes\n",
        "\n",
        "# ---------- user input, improvement, canonization and locking ----------\n",
        "topic_input = input(\"📘 Topic (few words): \").strip()\n",
        "char_input  = input(\"🎭 Main character (describe): \").strip()\n",
        "\n",
        "story_topic = topic_input or \"A Colourful Tale\"\n",
        "character_description = char_input or \"a playful kitten\"\n",
        "\n",
        "title, character_description = improve_seed(story_topic, character_description)\n",
        "canon_phrase, head_noun     = canonicalize_identity(title, character_description)\n",
        "\n",
        "# locking the head_noun\n",
        "canon_phrase = f\"the {head_noun}\".strip()\n",
        "\n",
        "print(f\"🔹 Title: {title}\")\n",
        "print(f\"🔹 Character: {character_description}\")\n",
        "print(f\"🔹 Canon phrase: {canon_phrase}  (noun: {head_noun})\")\n",
        "\n",
        "scenes = generate_scenes(title, character_description, canon_phrase, head_noun)\n",
        "\n",
        "print(\"\\n✅ 5 Scenes:\")\n",
        "for i, s in enumerate(scenes, 1):\n",
        "    print(f\"{i}. {s}\")\n"
      ],
      "metadata": {
        "id": "gtfQ1QzKbWUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============== Cell 4: reference and character identification (OWL-ViT Large→Base→CLIP→mid) ==============\n",
        "import os, re, torch, numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from IPython.display import display\n",
        "\n",
        "assert all(k in globals() for k in [\"character_description\",\"canon_phrase\",\"head_noun\"]), \\\n",
        "    \"❌ Run Cell 3 first - missing character_description/canon_phrase/head_noun\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ---------- עזרי שמירה/קרופ ----------\n",
        "def square_from_box(img, box, pad=1.15, out=256):\n",
        "    W,H = img.size\n",
        "    x1,y1,x2,y2 = map(float, box)\n",
        "    cx,cy = (x1+x2)/2,(y1+y2)/2\n",
        "    side = max(x2-x1, y2-y1) * pad\n",
        "    nx1,ny1 = max(0,int(cx-side/2)), max(0,int(cy-side/2))\n",
        "    nx2,ny2 = min(W,int(cx+side/2)), min(H,int(cy+side/2))\n",
        "    crop = img.crop((nx1,ny1,nx2,ny2))\n",
        "    return crop.resize((out,out), Image.LANCZOS), (nx1,ny1,nx2,ny2)\n",
        "\n",
        "# make reference image\n",
        "\n",
        "ref_image = None\n",
        "\"\"\"\n",
        "if use_reference:\n",
        "    if files:\n",
        "        print(\"📤 העלה תמונה (jpg/png):\")\n",
        "        up = files.upload()\n",
        "        if up: ref_image = Image.open(list(up.keys())[0]).convert(\"RGB\")\n",
        "    else:\n",
        "        p = input(\"🔎 נתיב לקובץ מקומי: \").strip()\n",
        "        if p and os.path.exists(p): ref_image = Image.open(p).convert(\"RGB\")\n",
        "\"\"\"\n",
        "if ref_image is None:\n",
        "    print(\"🖌️ creating reference image\")\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    sd_dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "    ref_pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        \"runwayml/stable-diffusion-v1-5\",\n",
        "        torch_dtype=sd_dtype,\n",
        "        safety_checker=None\n",
        "    ).to(device)\n",
        "    auto_prompt = (\n",
        "        f\"{canon_phrase}, close-up portrait, head and shoulders, looking at camera, \"\n",
        "        f\"centered, storybook illustration, soft natural lighting, high quality, clean background\"\n",
        "    )\n",
        "    neg = \"cropped, cut-off, out of frame, profile, side view, multiple subjects, watermark, text, lowres, blurry\"\n",
        "    gen = torch.Generator(device=device).manual_seed(12345)\n",
        "    ref_image = ref_pipe(prompt=auto_prompt, negative_prompt=neg,\n",
        "                         num_inference_steps=40, guidance_scale=8.0,\n",
        "                         generator=gen).images[0]\n",
        "    # ⚠️ erase and clean\n",
        "    del ref_pipe\n",
        "    if device == \"cuda\": torch.cuda.empty_cache()\n",
        "\n",
        "# ---------- 1) OWL-ViT: Free text character recognition ----------\n",
        "def owl_detect(img, labels, prefer_large=True):\n",
        "    from transformers import pipeline\n",
        "    model_id = \"google/owlvit-large-patch14\" if prefer_large else \"google/owlvit-base-patch32\"\n",
        "    det = pipeline(\"zero-shot-object-detection\", model=model_id,\n",
        "                   device=0 if device==\"cuda\" else -1)\n",
        "    # improve accuracy by resize for the long side to 896\n",
        "    W,H = img.size\n",
        "    scale = 896 / max(W,H)\n",
        "    base = img.resize((int(W*scale), int(H*scale)), Image.LANCZOS) if scale<1.0 else img\n",
        "    out = det(base, candidate_labels=labels)\n",
        "    if not out: return None\n",
        "    # give the one with the best score, resize back to original size\n",
        "    best = max(out, key=lambda d:d[\"score\"])\n",
        "    if best[\"score\"] < 0.15: return None # סף זיהוי\n",
        "    b = best[\"box\"]; s = best[\"score\"]\n",
        "    if scale<1.0:\n",
        "        inv = 1/scale\n",
        "        box = (b[\"xmin\"]*inv, b[\"ymin\"]*inv, b[\"xmax\"]*inv, b[\"ymax\"]*inv)\n",
        "    else:\n",
        "        box = (b[\"xmin\"], b[\"ymin\"], b[\"xmax\"], b[\"ymax\"])\n",
        "    return (box, s, model_id)\n",
        "\n",
        "noun = (head_noun or \"character\").strip()\n",
        "canon_clean = re.sub(r'^(a|an|the)\\s+','', canon_phrase, flags=re.IGNORECASE)\n",
        "labels = []\n",
        "for t in [canon_clean, noun, f\"{noun} face\", f\"{noun} head\", f\"{noun} portrait\",\n",
        "          f\"{noun} full body\", \"main character\", \"single character\"]:\n",
        "    t = t.strip()\n",
        "    if t and t.lower() not in {x.lower() for x in labels}: labels.append(t)\n",
        "\n",
        "full_512 = ref_image.resize((512,512), Image.LANCZOS)\n",
        "\n",
        "box, score, mid = None, None, None\n",
        "try:\n",
        "    r = owl_detect(full_512, labels, prefer_large=True)\n",
        "    if r and r[1] >= 0.20: box, score, mid = r\n",
        "    else:\n",
        "        r = owl_detect(full_512, labels, prefer_large=False)\n",
        "        if r and r[1] >= 0.18: box, score, mid = r\n",
        "except Exception as e:\n",
        "    print(\"⚠️ details: CLIP-crop unavailable, try OWL-ViT\", e)\n",
        "    method = None\n",
        "\n",
        "# ---------- 2) fail: multi-scale CLIP-crop ----------\n",
        "if box is None:\n",
        "    try:\n",
        "        import open_clip\n",
        "        model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-L-14\", pretrained=\"openai\")\n",
        "        model = model.to(device).eval()\n",
        "        tok = open_clip.get_tokenizer(\"ViT-L-14\")\n",
        "        def enc_t(t):\n",
        "            t = re.sub(r\"\\s+\",\" \", t).strip()[:140]\n",
        "            with torch.no_grad():\n",
        "                toks = tok([t]).to(device)\n",
        "                v = model.encode_text(toks); return v / v.norm(dim=-1, keepdim=True)\n",
        "        def enc_i(pil):\n",
        "            with torch.no_grad():\n",
        "                ii = preprocess(pil).unsqueeze(0).to(device)\n",
        "                v = model.encode_image(ii); return v / v.norm(dim=-1, keepdim=True)\n",
        "        tfeat = enc_t(f\"close-up {noun} face\" if noun else \"close-up face\")\n",
        "        W,H = full_512.size; best = (-1,(0,0,224,224),224)\n",
        "        for cs in (176,208,240,272,304):\n",
        "            base = full_512\n",
        "            if W < cs or H < cs:\n",
        "                scale = max(cs/W, cs/H)\n",
        "                base = full_512.resize((int(W*scale)+2, int(H*scale)+2), Image.LANCZOS)\n",
        "            BW, BH = base.size\n",
        "            for y in range(0, BH - cs + 1, 16):\n",
        "                for x in range(0, BW - cs + 1, 16):\n",
        "                    patch = base.crop((x,y,x+cs,y+cs))\n",
        "                    sc = torch.cosine_similarity(enc_i(patch), tfeat).item()\n",
        "                    if sc > best[0]: best = (sc,(x,y,x+cs,y+cs),cs)\n",
        "        box, score, mid = best[1], best[0], \"open_clip\"\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ CLIP-crop unavailable\", e)\n",
        "\n",
        "# ---------- 3) last fail ----------\n",
        "method = None\n",
        "if box is None:\n",
        "    W,H = full_512.size\n",
        "    side = int(min(W,H)*0.36)\n",
        "    cx,cy = W//2, int(H*0.42)\n",
        "    box = (cx-side, cy-side, cx+side, cy+side)\n",
        "    method = \"center\"\n",
        "else:\n",
        "    method = f\"{mid} ({score:.3f})\"\n",
        "\n",
        "# ---------- 4) save ----------\n",
        "ref_focus, used_box = square_from_box(full_512, box, pad=1.15, out=256)\n",
        "full_512.save(\"reference_full.png\")\n",
        "ref_focus.save(\"reference_focus.png\")\n",
        "\n",
        "viz = full_512.copy()\n",
        "draw = ImageDraw.Draw(viz)\n",
        "draw.rectangle(tuple(map(int, used_box)), outline=(255,0,0), width=4)\n",
        "\n",
        "print(f\"✅ נשמרו קבצי רפרנס — שיטת קרופ: {method}\")\n",
        "print(\"   • reference_full.png (512×512)\")\n",
        "print(\"   • reference_focus.png (256×256)\")\n",
        "display(viz)\n",
        "display(ref_focus)"
      ],
      "metadata": {
        "id": "XMpImJmQPOiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "HbWRB1s7boLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== Cell 5 — SD + IP-Adapter + Text-first Ranking (Ref-boosted grid) ======================\n",
        "import os, re, json, random, shutil\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# ---------- Setup ----------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "assert \"scenes\" in globals() and isinstance(scenes, list) and len(scenes) == 5, \"❌ missing scenes from Cell 3\"\n",
        "title  = globals().get(\"title\",  \"Untitled Story\")\n",
        "canon_phrase = globals().get(\"canon_phrase\", \"the character\")\n",
        "head_noun    = globals().get(\"head_noun\", \"character\")\n",
        "\n",
        "# Reference image (from Cell 4)\n",
        "if \"ref_image\" not in globals() or ref_image is None:\n",
        "    if os.path.exists(\"ref_image.png\"):\n",
        "        ref_image = Image.open(\"ref_image.png\").convert(\"RGB\")\n",
        "    else:\n",
        "        raise RuntimeError(\"❌ missing reference image\")\n",
        "\n",
        "# Style / negatives (fallbacks if not defined)\n",
        "STYLE = globals().get(\"STYLE\",\n",
        "    \"storybook illustration, soft natural light, vibrant yet gentle colors, clean background, high quality\"\n",
        ")\n",
        "NEG_COMMON = globals().get(\"NEG_COMMON\",\n",
        "    \"lowres, blurry, noisy, watermark, text, cut-off, cropped, out of frame, duplicate, multiple subjects\"\n",
        ")\n",
        "\n",
        "# ---------- Pipeline with IP-Adapter ----------\n",
        "from diffusers import StableDiffusionPipeline\n",
        "if \"pipe\" not in globals() or pipe is None:\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        \"runwayml/stable-diffusion-v1-5\",\n",
        "        torch_dtype=torch_dtype,\n",
        "        safety_checker=None\n",
        "    ).to(device)\n",
        "    print(\"✅ SD v1.5 pipeline loaded.\")\n",
        "\n",
        "if not hasattr(pipe, \"_ip_adapter_loaded\"):\n",
        "    try:\n",
        "        pipe.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.bin\")\n",
        "        pipe._ip_adapter_loaded = True\n",
        "        print(\"✅ IP-Adapter loaded for SD1.5.\")\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ Failed to load IP-Adapter:\", e)\n",
        "\n",
        "def set_ip_adapter_scale(pipe, scale: float):\n",
        "    if hasattr(pipe, \"set_ip_adapter_scale\"):\n",
        "        pipe.set_ip_adapter_scale(scale)\n",
        "    elif hasattr(pipe, \"ip_adapter_scale\"):\n",
        "        pipe.ip_adapter_scale = scale\n",
        "    else:\n",
        "        raise RuntimeError(\"IP-Adapter scale API not found on this pipeline build.\")\n",
        "\n",
        "# ---------- Embedding backends for ranking ----------\n",
        "oc_model = oc_preprocess = oc_tokenizer = None\n",
        "clip_model = clip_preprocess = None\n",
        "try:\n",
        "    import open_clip\n",
        "    oc_model, _, oc_preprocess = open_clip.create_model_and_transforms(\n",
        "        \"ViT-H-14\", pretrained=\"laion2b_s32b_b79k\", device=device\n",
        "    )\n",
        "    oc_model.eval()\n",
        "    oc_tokenizer = open_clip.get_tokenizer(\"ViT-H-14\")\n",
        "    print(\"✅ open_clip (ViT-H-14) ready.\")\n",
        "except Exception:\n",
        "    try:\n",
        "        import clip\n",
        "        clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "        clip_model.eval()\n",
        "        print(\"✅ CLIP ViT-B/32 ready.\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"❌ Neither open_clip nor CLIP available.\") from e\n",
        "\n",
        "@torch.inference_mode()\n",
        "def encode_image_feat(img: Image.Image):\n",
        "    if oc_model is not None:\n",
        "        t = oc_preprocess(img).unsqueeze(0).to(device)\n",
        "        feat = oc_model.encode_image(t).float()\n",
        "    else:\n",
        "        t = clip_preprocess(img).unsqueeze(0).to(device)\n",
        "        feat = clip_model.encode_image(t).float()\n",
        "    return feat / feat.norm(dim=-1, keepdim=True)\n",
        "\n",
        "@torch.inference_mode()\n",
        "def encode_text_feat(text: str):\n",
        "    s = re.sub(r\"\\s+\", \" \", text.strip())\n",
        "    if oc_model is not None:\n",
        "        tokens = oc_tokenizer([s]).to(device)\n",
        "        feat = oc_model.encode_text(tokens).float()\n",
        "    else:\n",
        "        import clip as _clip\n",
        "        tokens = _clip.tokenize([s]).to(device)\n",
        "        feat = clip_model.encode_text(tokens).float()\n",
        "    return feat / feat.norm(dim=-1, keepdim=True)\n",
        "\n",
        "ref_feat = encode_image_feat(ref_image) if ref_image is not None else None\n",
        "\n",
        "# ---------- Ranking: text-first, ref tie-break ----------\n",
        "@torch.inference_mode()\n",
        "def rank_text_first(text: str, paths, ref_feat=None, k=4):\n",
        "    \"\"\"\n",
        "    1) Rank all by text↔image; take top-k by text.\n",
        "    2) Among top-k, pick highest by ref↔image (if ref exists), else best-by-text.\n",
        "    Returns (best_path, all_text_scores_list).\n",
        "    \"\"\"\n",
        "    tfeat = encode_text_feat(text)\n",
        "    scored = []\n",
        "    for p in paths:\n",
        "        img = Image.open(p).convert(\"RGB\")\n",
        "        emb = encode_image_feat(img)\n",
        "        s_txt = torch.matmul(emb, tfeat.T).item()\n",
        "        scored.append((s_txt, p, emb))\n",
        "\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    top = scored[:min(k, len(scored))]\n",
        "    if not top:\n",
        "        return (max(scored)[1] if scored else None), [s for s,_,_ in scored]\n",
        "\n",
        "    if ref_feat is None:\n",
        "        return top[0][1], [s for s,_,_ in scored]\n",
        "\n",
        "    best_p, best_sr = top[0][1], -1e9\n",
        "    for s_txt, p, emb in top:\n",
        "        s_ref = torch.matmul(emb, ref_feat.T).item()\n",
        "        if s_ref > best_sr:\n",
        "            best_sr, best_p = s_ref, p\n",
        "    return best_p, [s for s,_,_ in scored]\n",
        "\n",
        "# ---------- Scales & layout (Ref-boosted grid) ----------\n",
        "def ip_scale_base(shot):\n",
        "    # Slightly stronger than the softer variant to let identity read, still scene-friendly\n",
        "    return 0.18 if shot == \"wide\" else 0.26\n",
        "\n",
        "def variant_grid(base, shot):\n",
        "    \"\"\"\n",
        "    Build a small grid of (ip_adapter_scale, guidance_scale) with a gentle ref boost.\n",
        "    Wider shots get a lower cap; medium/portrait can push a bit higher.\n",
        "    \"\"\"\n",
        "    cap = 0.32 if shot == \"wide\" else 0.42    # shot-aware cap\n",
        "    hi  = min(cap, base * 1.35)               # higher identity candidate\n",
        "    mid = min(cap, (base + hi) / 2)           # mid candidate\n",
        "\n",
        "    return [\n",
        "        (0.06,                    7.5),  # near text-only\n",
        "        (max(0.10, base*0.7),     7.5),\n",
        "        (base,                    7.8),\n",
        "        (mid,                     7.8),\n",
        "        (hi,                      8.0),  # gentle ref boost\n",
        "    ]\n",
        "\n",
        "def decide_plan(scene_text: str):\n",
        "    st = scene_text.lower()\n",
        "    shot = \"wide\" if any(k in st for k in [\"field\",\"park\",\"meadow\",\"street\",\"shore\",\"sky\",\"hills\",\"valley\",\"landscape\"]) else \"medium\"\n",
        "    orient = \"landscape\" if any(k in st for k in [\"wide\",\"horizon\",\"hills\",\"valley\",\"street\",\"shore\",\"sea\",\"sky\"]) else \"portrait\"\n",
        "    return shot, orient\n",
        "\n",
        "def size_for(orient: str):\n",
        "    if orient == \"landscape\":\n",
        "        return 768, 576\n",
        "    elif orient == \"portrait\":\n",
        "        return 576, 768\n",
        "    else:\n",
        "        return 640, 640\n",
        "\n",
        "# ---------- Generate & select ----------\n",
        "random.seed(1234); np.random.seed(1234)\n",
        "os.makedirs(\"scene_variants\", exist_ok=True)\n",
        "os.makedirs(\"scene_best\", exist_ok=True)\n",
        "\n",
        "best_scores = {}\n",
        "all_best = []\n",
        "\n",
        "for idx, scene in enumerate(scenes, 1):\n",
        "    scene_short = re.sub(r\"\\s+\", \" \", scene).strip()\n",
        "    shot, orient = decide_plan(scene_short)\n",
        "    width, height = size_for(orient)\n",
        "    base = ip_scale_base(shot)\n",
        "    grid = variant_grid(base, shot)\n",
        "\n",
        "    # Ranking text uses ONLY the scene sentence (no style/shot tokens)\n",
        "    rank_text = scene_short\n",
        "    sd_prompt = f\"{scene_short}, {STYLE}\"\n",
        "    neg = NEG_COMMON\n",
        "\n",
        "    gen_paths = []\n",
        "    for k_idx, (ip_sc, guidance) in enumerate(grid, 1):\n",
        "        try:\n",
        "            set_ip_adapter_scale(pipe, ip_sc)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"IP-Adapter scale set failed: {e}\")\n",
        "\n",
        "        seed = 1000*idx + k_idx\n",
        "        generator = torch.Generator(device=device).manual_seed(seed)\n",
        "\n",
        "        img = pipe(\n",
        "            prompt=sd_prompt,\n",
        "            negative_prompt=neg,\n",
        "            ip_adapter_image=ref_image,\n",
        "            num_inference_steps=28,\n",
        "            guidance_scale=guidance,\n",
        "            width=width, height=height,\n",
        "            generator=generator\n",
        "        ).images[0]\n",
        "\n",
        "        outp = Path(\"scene_variants\") / f\"scene{idx:02d}_v{k_idx:02d}.png\"\n",
        "        img.save(outp)\n",
        "        gen_paths.append(str(outp))\n",
        "\n",
        "    # Text-first ranking; break ties by reference among top-k\n",
        "    best_path, text_scores = rank_text_first(rank_text, gen_paths, ref_feat=ref_feat, k=4)\n",
        "    best_scores[idx] = float(max(text_scores)) if text_scores else float(\"nan\")\n",
        "\n",
        "    dest = Path(\"scene_best\") / f\"scene_{idx}.png\"\n",
        "    shutil.copy2(best_path, dest)\n",
        "    all_best.append(str(dest))\n",
        "    print(f\"✅ Scene {idx}: chose {Path(best_path).name}  | text-score≈{best_scores[idx]:.4f}\")\n",
        "    plt.imshow(Image.open(best_path)); plt.axis(\"off\"); plt.title(f\"Best for Scene {idx}\"); plt.show()\n",
        "\n",
        "# Save simple metrics\n",
        "with open(\"best_scores.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(best_scores, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"\\n🎉 Done. Best images saved in scene_best/.  Ranking = text-first (ref tie-break).\")\n"
      ],
      "metadata": {
        "id": "sPuQwLm7eonj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== Cell6 - export the story (HTML + JSON) ======================\n",
        "import os, json, html, base64, re\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# --- Preconditions ---\n",
        "assert \"title\" in globals() and \"character_description\" in globals(), \"❌ חסרים title/character_description.\"\n",
        "assert \"scenes\" in globals() and isinstance(scenes, list) and len(scenes) == 5, \"❌ missing scenes\"\n",
        "assert os.path.isdir(\"scene_best\"), \"❌ run Cell 5\"\n",
        "\n",
        "# --- Reference image: save / locate ---\n",
        "ref_path = None\n",
        "try:\n",
        "    if \"ref_image\" in globals() and ref_image is not None:\n",
        "        ref_image = ref_image.convert(\"RGB\")\n",
        "        ref_path = \"ref_image.png\"\n",
        "        ref_image.save(ref_path)\n",
        "    elif os.path.exists(\"ref_image.png\"):\n",
        "        ref_path = \"ref_image.png\"\n",
        "except Exception as e:\n",
        "    print(\"⚠️ Failed to save ref_image.png:\", e)\n",
        "    ref_path = \"ref_image.png\" if os.path.exists(\"ref_image.png\") else None\n",
        "\n",
        "# --- JSON meta export ---\n",
        "meta = {\n",
        "    \"title\": title,\n",
        "    \"character\": character_description,\n",
        "    \"canon_phrase\": canon_phrase,\n",
        "    \"head_noun\": head_noun,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"rank_backend\": RANK_BACKEND if \"RANK_BACKEND\" in globals() else \"n/a\",\n",
        "    \"best_scores\": best_scores if \"best_scores\" in globals() else {},\n",
        "    \"scenes\": [\n",
        "        {\"index\": i, \"text\": scenes[i-1], \"image\": f\"scene_best/scene_{i}.png\"}\n",
        "        for i in range(1, 6)\n",
        "    ],\n",
        "    \"created_at\": datetime.utcnow().isoformat() + \"Z\"\n",
        "}\n",
        "if ref_path:\n",
        "    meta[\"ref_image\"] = ref_path\n",
        "\n",
        "with open(\"story.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# --- HTML document ---\n",
        "def esc(x): return html.escape(x, quote=True)\n",
        "\n",
        "html_body = []\n",
        "html_body.append(f\"<h1>{esc(title)}</h1>\")\n",
        "html_body.append(f\"<p><b>Main character:</b> {esc(character_description)}</p>\")\n",
        "\n",
        "# Reference image BEFORE Scene 1\n",
        "if ref_path:\n",
        "    html_body.append(f\"\"\"\n",
        "    <figure class=\"reference\">\n",
        "      <img src=\"{esc(ref_path)}\" alt=\"reference image\" />\n",
        "      <figcaption>Reference image</figcaption>\n",
        "    </figure>\n",
        "    \"\"\".strip())\n",
        "\n",
        "# Scenes 1..5\n",
        "for i in range(1, 6):\n",
        "    txt = scenes[i-1]\n",
        "    img = f\"scene_best/scene_{i}.png\"\n",
        "    score = (best_scores.get(i) if 'best_scores' in globals() else None)\n",
        "    html_body.append(f\"\"\"\n",
        "    <div class=\"scene\">\n",
        "      <h2>Scene {i}{' — CLIP {:.3f}'.format(score) if (score is not None) else ''}</h2>\n",
        "      <p>{esc(txt)}</p>\n",
        "      <img src=\"{esc(img)}\" alt=\"scene {i}\" />\n",
        "    </div>\n",
        "    \"\"\".strip())\n",
        "\n",
        "html_doc = f\"\"\"\n",
        "<!doctype html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "<meta charset=\"utf-8\" />\n",
        "<title>{esc(title)}</title>\n",
        "<style>\n",
        "body {{ font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif; margin: 24px; }}\n",
        "h1 {{ margin-bottom: 0.2rem; }}\n",
        "p {{ line-height: 1.5; }}\n",
        ".reference {{ margin: 18px 0 28px; text-align: center; }}\n",
        ".reference img {{ max-width: 100%; height: auto; border-radius: 12px; box-shadow: 0 6px 24px rgba(0,0,0,0.12); }}\n",
        ".reference figcaption {{ font-size: 0.9rem; color: #666; margin-top: 6px; }}\n",
        ".scene {{ margin: 28px 0; }}\n",
        ".scene img {{ max-width: 100%; height: auto; border-radius: 12px; box-shadow: 0 6px 24px rgba(0,0,0,0.15); }}\n",
        "footer {{ margin-top: 24px; color: #777; font-size: 0.9rem; }}\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "{''.join(html_body)}\n",
        "<footer>\n",
        "  <p>Generated on {esc(meta['created_at'])}</p>\n",
        "</footer>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\".strip()\n",
        "\n",
        "with open(\"storybook.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(html_doc)\n",
        "\n",
        "# --- Single-file HTML: embed ALL images (ref + scenes) as base64 data URIs ---\n",
        "def embed_image(html_text: str, path: str) -> str:\n",
        "    \"\"\"Replace src='path' with a data: URI, if file exists.\"\"\"\n",
        "    if not (path and os.path.exists(path)):\n",
        "        print(f\"⚠️ Missing image for embedding: {path}\")\n",
        "        return html_text\n",
        "    ext = Path(path).suffix.lower().lstrip(\".\")\n",
        "    mime = \"image/png\" if ext in (\"png\", \"\") else (\"image/jpeg\" if ext in (\"jpg\", \"jpeg\") else f\"image/{ext}\")\n",
        "    with open(path, \"rb\") as imgf:\n",
        "        b64 = base64.b64encode(imgf.read()).decode(\"ascii\")\n",
        "    return html_text.replace(f'src=\"{path}\"', f'src=\"data:{mime};base64,{b64}\"')\n",
        "\n",
        "# Read the generated HTML\n",
        "with open(\"storybook.html\", \"r\", encoding=\"utf-8\") as f:\n",
        "    single = f.read()\n",
        "\n",
        "# Embed scenes\n",
        "for i in range(1, 6):\n",
        "    single = embed_image(single, f\"scene_best/scene_{i}.png\")\n",
        "\n",
        "# Embed reference (if present)\n",
        "if ref_path:\n",
        "    single = embed_image(single, ref_path)\n",
        "\n",
        "# Remove any <base> if present (not used here, but safe)\n",
        "single = re.sub(r'<base[^>]*>', '', single, flags=re.I)\n",
        "\n",
        "with open(\"storybook_single.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(single)\n",
        "\n",
        "print(\"✅ Saved files:\")\n",
        "print(\" • story.json\")\n",
        "print(\" • storybook.html  (uses relative image files)\")\n",
        "print(\" • storybook_single.html  (all images embedded)\")\n",
        "if ref_path:\n",
        "    print(f\" • {ref_path}\")\n"
      ],
      "metadata": {
        "id": "zOHn-BicLplp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}