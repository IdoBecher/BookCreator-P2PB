# BookCreator-P2PB ‚Äî From Prompt to Picture-Book  
*An end-to-end, reproducible pipeline for five-scene picture-book generation using SD 1.5 + IP-Adapter with CLIP-based selection.*

> **Goal.** Keep the **same main character** recognizable across scenes, while **layouts stay diverse** and each image matches its **scene caption**.

---

## üåê Live demo (GitHub Pages)

- Project hub: **https://idobecher.github.io/BookCreator-P2PB/**  

Storybook examples (HTML):
- `docs/rainbow-kittens-adventure.html`  
- `docs/puppy-playdate.html`  
- `docs/robot-waiter-at-the-table.html`

---

## ‚ú® What this repo contains

1. **LLM text module** ‚Äì refines title/character and generates **exactly 5** short, visual scenes (with retries + de-dup).
2. **Reference module** ‚Äì creates/accepts a **reference portrait** and extracts a **focal crop** (category-agnostic) to guide identity.
3. **Rendering** ‚Äì Stable Diffusion **v1.5** + **IP-Adapter (SD15)**, conditioning on **both** reference images (focus + full) with **shot-aware** scales.
4. **Selection** ‚Äì samples a small grid and picks the best per scene using **OpenAI CLIP** cosine (text-first) with **reroll** when scores are low.
5. **Export** ‚Äì saves `scene_best/scene_1..5.png`, `story.json`, and a **storybook HTML** (+ optional PDF).
6. **Publish** ‚Äì everything in `docs/` is served on **GitHub Pages**.


---

## üß≠ Repository layout

```text
BookCreator-P2PB/
‚îú‚îÄ code/
‚îÇ  ‚îî‚îÄ storybook.ipynb      # end-to-end Colab/Local notebook
‚îú‚îÄ docs/                           # GitHub Pages site
‚îÇ  ‚îú‚îÄ index.html                    # Simple gallery/landing page
‚îÇ  ‚îú‚îÄ puppy-playdate.html
‚îÇ  ‚îú‚îÄ puppy-playdate.pdf
‚îÇ  ‚îú‚îÄ puppy-playdate.png
‚îÇ  ‚îú‚îÄ rainbow-kittens-adventure.html
‚îÇ  ‚îú‚îÄ rainbow-kittens-adventure.pdf
‚îÇ  ‚îú‚îÄ rainbow-kittens-adventure.png
‚îÇ  ‚îú‚îÄ robot-waiter-at-the-table.html
‚îÇ  ‚îú‚îÄ robot-waiter-at-the-table.pdf
‚îÇ  ‚îî‚îÄ robot-waiter-at-the-table.png
‚îî‚îÄ README.md
```


---

## üì¶ Installation

```bash
# Python 3.10+ recommended, CUDA if available
pip install -U diffusers transformers accelerate torch torchvision   sentencepiece safetensors opencv-python-headless   git+https://github.com/openai/CLIP.git
```

**Model weights**
- **Stable Diffusion v1.5** ‚Äî `runwayml/stable-diffusion-v1-5`  
- **IP-Adapter (SD15)** ‚Äî `h94/IP-Adapter` (`ip-adapter_sd15.bin`)  
- **CLIP** ‚Äî OpenAI CLIP ViT-L/14 (fallback ViT-B/32)  
- **(Optional) OWL-ViT** for zero-shot focal detection

> If a Hugging Face repo is gated (401/403), run `huggingface-cli login` or set `HF_TOKEN`.

---

## ‚ñ∂Ô∏è Quickstart

### A) Colab (recommended)
1. Open **`notebooks/BookCreator_final.ipynb`** (GPU runtime).  
2. Run top-to-bottom.  
3. You‚Äôll get:
   - `reference_full.png`, `reference_focus.png`
   - `scene_best/scene_1..5.png`
   - `story.json`, `storybook.html` (+ optional PDF)

### B) Local CLI (optional)
```bash
python -m code.render_scenes   --topic "robot waiter at the table"   --character "a friendly red service robot"   --outdir outputs/robot_waiter
```

---

## üß± How it works (high level)

**Dual-reference IP-Adapter**  
We condition on **two** images:
- `reference_focus.png` (256√ó256): stabilizes face/details.  
- `reference_full.png` (512√ó512): carries color/pattern/composition cues.  

**Shot-aware scales (examples):** wide‚âà0.18 (cap‚â§0.32), medium/portrait‚âà0.26 (cap‚â§0.42).  
Per scene we render a small grid of `(ip_scale, guidance)` variants.

**Generic focal crop (category-agnostic)**  
1) **OWL-ViT** zero-shot proposals from the head noun (e.g., ‚Äúkitten/head/face/full body‚Äù).  
2) If weak ‚Üí **CLIP sliding windows** over multiple window sizes; pick max cosine vs a compact ‚Äúclose-up face of ‚Ä¶‚Äù text.  
3) Else soft **center** fallback.  
Works for cats/dogs/robots/toys/etc. (no cat-/human-specific face detector required).

**CLIP-aware prompts**  
Truncate scene text for both SD and CLIP to stay **<77 tokens**; keep key nouns/attributes.

**Selection: text-first + reference tie-break**  
Rank by text‚Üíimage similarity, keep Top-K, break ties by reference‚Üíimage similarity.  
‚Üí **Scene fidelity** + **identity consistency** together.

**Reroll when low**  
If best CLIP score < œÑ (e.g., ~0.27 for ViT-L/14), rerender 2‚Äì3 extra candidates with **slightly lower IP-scale** to increase composition diversity.

---


## üß™ Notes & tips

- **Colab constraints**: prefer fp16 on GPU; don‚Äôt move fp16 pipelines to CPU. If you load with `device_map="auto"`, don‚Äôt pass `device=` to `pipeline()`.
- **Gated repos**: 401/403 ‚Üí authenticate (`HF_TOKEN`) or choose public weights.
- **CLIP 77 tokens**: always truncate scene texts for CLIP/SD; keep key nouns/attributes.
- **Duplicate subjects**: strengthen negatives (`duplicate, twins, many, watermark, text`).
- **Large files on GitHub**: for Pages/README previews use compressed **thumbnails** (PNG/WebP, ‚â§1600px). Heavy PDFs/images ‚Üí compress or attach via **Releases** (or Git LFS).

---

## üìö References

- Rombach et al., *High-Resolution Image Synthesis with Latent Diffusion Models* (CVPR‚Äô22)  
- Ye et al., *IP-Adapter: Text Compatible Image Prompt Adapter* (arXiv‚Äô23)  
- Radford et al., *CLIP: Learning Transferable Visual Models from Natural Language Supervision* (ICML‚Äô21)  
- Minderer et al., *OWL-ViT: Simple Open-Vocabulary Object Detection with Vision Transformers* (ECCV‚Äô22)  
- Hugging Face: **diffusers**, **transformers**

---


## ‚ú® Project status

Final notebook + published storybooks.  
Contributions (typo fixes / docs / small robustness patches) are welcome.
